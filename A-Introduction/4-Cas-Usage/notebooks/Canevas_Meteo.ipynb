{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TP d'analyse de la météo\n",
    "\n",
    "Sur le site weatherunderground, nous avons récupéré la météo pour les éléments suivants:\n",
    "\n",
    "    villes = ['Paris', 'Marseille', 'Grenoble', 'Lille', 'Strasbourg', 'Nantes', 'Bordeaux']\n",
    "    year   = '2017'\n",
    "    month  = ['01', '02', '03', '04', '05', '06', '07', '08', '09', '10', '11', '12']\n",
    "    day    = ['01', '08', '15', '22']\n",
    "\n",
    "[Pour info] Le script de constitution du corpus est récupérable à l'adresse : http://www-connex.lip6.fr/~guigue/weatherdata.py <br>\n",
    "Pour le TP, les données sont à télécharger à l'adresse : http://www-connex.lip6.fr/~guigue/weatherJSON.pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# definition des import nécessaires\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import pickle as pkl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chargement des données + affichage basique\n",
    "Code fourni\n",
    "\n",
    "1. Chargement des données pickle\n",
    "1. Exploration des données disponibles par affichages multiples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chemin vers le .pkl que vous aurez préalablement téléchargé:\n",
    "path2file = \"ressources/weatherJSON.pkl\"\n",
    "\n",
    "data = pkl.load(open(path2file, \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "336\n",
      "2\n",
      "dict_keys(['response', 'history'])\n",
      "dict_keys(['version', 'termsofService', 'features'])\n",
      "dict_keys(['date', 'utcdate', 'observations', 'dailysummary'])\n"
     ]
    }
   ],
   "source": [
    "# analyse basique:\n",
    "print(len(data)) # 336 = nb_villes x nb_mois x nb_jours\n",
    "print(len(data[0])) # Chaque entrée est composée de deux choses\n",
    "print(data[0].keys())\n",
    "print(data[0]['response'].keys()) # listes des clés du premier dico associé à chaque entrée\n",
    "# => 'response' n'a pas l'air intéressante\n",
    "\n",
    "print(data[0]['history'].keys()) # listes des clés du deuxième dico associé à chaque entrée\n",
    "# => Beaucoup mieux... On poursuit l'exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['date', 'fog', 'rain', 'snow', 'snowfallm', 'snowfalli', 'monthtodatesnowfallm', 'monthtodatesnowfalli', 'since1julsnowfallm', 'since1julsnowfalli', 'snowdepthm', 'snowdepthi', 'hail', 'thunder', 'tornado', 'meantempm', 'meantempi', 'meandewptm', 'meandewpti', 'meanpressurem', 'meanpressurei', 'meanwindspdm', 'meanwindspdi', 'meanwdire', 'meanwdird', 'meanvism', 'meanvisi', 'humidity', 'maxtempm', 'maxtempi', 'mintempm', 'mintempi', 'maxhumidity', 'minhumidity', 'maxdewptm', 'maxdewpti', 'mindewptm', 'mindewpti', 'maxpressurem', 'maxpressurei', 'minpressurem', 'minpressurei', 'maxwspdm', 'maxwspdi', 'minwspdm', 'minwspdi', 'maxvism', 'maxvisi', 'minvism', 'minvisi', 'gdegreedays', 'heatingdegreedays', 'coolingdegreedays', 'precipm', 'precipi', 'precipsource', 'heatingdegreedaysnormal', 'monthtodateheatingdegreedays', 'monthtodateheatingdegreedaysnormal', 'since1sepheatingdegreedays', 'since1sepheatingdegreedaysnormal', 'since1julheatingdegreedays', 'since1julheatingdegreedaysnormal', 'coolingdegreedaysnormal', 'monthtodatecoolingdegreedays', 'monthtodatecoolingdegreedaysnormal', 'since1sepcoolingdegreedays', 'since1sepcoolingdegreedaysnormal', 'since1jancoolingdegreedays', 'since1jancoolingdegreedaysnormal'])\n",
      "{'date': {'pretty': '12:00 AM CET on January 01, 2017', 'year': '2017', 'mon': '01', 'mday': '01', 'hour': '00', 'min': '00', 'tzname': 'Europe/Paris'}, 'fog': '0', 'rain': '0', 'snow': '0', 'snowfallm': '', 'snowfalli': '', 'monthtodatesnowfallm': '', 'monthtodatesnowfalli': '', 'since1julsnowfallm': '', 'since1julsnowfalli': '', 'snowdepthm': '', 'snowdepthi': '', 'hail': '0', 'thunder': '0', 'tornado': '0', 'meantempm': '-2', 'meantempi': '28', 'meandewptm': '-3', 'meandewpti': '26', 'meanpressurem': '1023.18', 'meanpressurei': '30.22', 'meanwindspdm': '10', 'meanwindspdi': '6', 'meanwdire': 'South', 'meanwdird': '186', 'meanvism': '7.5', 'meanvisi': '4.6', 'humidity': '90', 'maxtempm': '0', 'maxtempi': '32', 'mintempm': '-4', 'mintempi': '24', 'maxhumidity': '93', 'minhumidity': '83', 'maxdewptm': '-1', 'maxdewpti': '30', 'mindewptm': '-6', 'mindewpti': '21', 'maxpressurem': '1029', 'maxpressurei': '30.40', 'minpressurem': '1020', 'minpressurei': '30.12', 'maxwspdm': '17', 'maxwspdi': '10', 'minwspdm': '6', 'minwspdi': '4', 'maxvism': '12.0', 'maxvisi': '7.0', 'minvism': '4.0', 'minvisi': '2.0', 'gdegreedays': '0', 'heatingdegreedays': '37', 'coolingdegreedays': '0', 'precipm': '0.0', 'precipi': '0.00', 'precipsource': 'Precip6GroupSynop', 'heatingdegreedaysnormal': '', 'monthtodateheatingdegreedays': '', 'monthtodateheatingdegreedaysnormal': '', 'since1sepheatingdegreedays': '', 'since1sepheatingdegreedaysnormal': '', 'since1julheatingdegreedays': '', 'since1julheatingdegreedaysnormal': '', 'coolingdegreedaysnormal': '', 'monthtodatecoolingdegreedays': '', 'monthtodatecoolingdegreedaysnormal': '', 'since1sepcoolingdegreedays': '', 'since1sepcoolingdegreedaysnormal': '', 'since1jancoolingdegreedays': '', 'since1jancoolingdegreedaysnormal': ''}\n"
     ]
    }
   ],
   "source": [
    "# exploration d'une entrée:\n",
    "print(data[0]['history']['dailysummary'][0].keys())\n",
    "print(data[0]['history']['dailysummary'][0]) # nous allons nous concentrer sur les résumés quotidiens\n",
    "# => les infos dans 'observations' sont plus précises... Mais on les laisse de coté"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Les vérrous à lever pour pouvoir traiter les données:\n",
    "\n",
    "### Q1. On remarque qu'il y a des dictionnaires dans des dictionaires...\n",
    "Il faut tout remettre à plat pour une importation facile.<br>\n",
    "La méthode pd.io.json.json_normalize fait ça ! \n",
    "\n",
    "    df = pd.io.json.json_normalize(data[0]['history']['dailysummary'][0])\n",
    "\n",
    "### Q2. On a une liste de JSON (liée à la manière dont les données ont été collectées)\n",
    "=> On veut remettre tout ça dans un tableau <br>\n",
    "Il faut parcourir tous les i entre 0 et 335 :\n",
    "    \n",
    "    data[i]['history']['dailysummary'][0]\n",
    "    \n",
    "Et ajouter des lignes selon la procédure:\n",
    "\n",
    "    df = df.append( DataFrame object)\n",
    "\n",
    "### Q3. L'information sur la ville a été malencontreusement égarée... Il faut la remettre dans la table\n",
    "Etant donné les villes dans l'ordre :\n",
    "    \n",
    "    villes = ['Paris', 'Marseille', 'Grenoble', 'Lille', 'Strasbourg', 'Nantes', 'Bordeaux']\n",
    "    coords = [[48.853340,2.348342], [43.295421,5.374743],[45.185850, 5.736848],\\\n",
    "                  [50.636907,3.068034], [48.580519, 7.751759], [47.215708, -1.553418], [44.841386,-0.570341]]\n",
    "\n",
    "Les 48 premières entrées correspondent à Paris, les 48 suivantes à Marseille etc...\n",
    "En pratique, vous ajouterez 4 colonnes:\n",
    "1. colonne 'ville' contenant la chaine décrivant la ville\n",
    "1. colonne 'indexville' contenant un entier de 0 à 6 correspondant à la ville\n",
    "1. colonne 'latitude' contenant la première coordonnée\n",
    "\n",
    "    Ces informations sont dupliquées pour toutes les lignes... On ne s'embete pas avec des jointures\n",
    "1. colonne 'longitude' contenant la seconde coordonnée\n",
    "\n",
    "Implémentation: la génération de ces colonnes est un bon exercice de révision des boucles.\n",
    "\n",
    "Rappel:\n",
    "* Créer une liste contenant N fois une valeur v:\n",
    "    \n",
    "     li = [v]*N\n",
    "     \n",
    "* Concaténer deux listes\n",
    "\n",
    "    li = li1 + li2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mise en forme des données (remise à plat des dicos dans les dicos)\n",
    "df = pd.io.json.json_normalize(data[0]['history']['dailysummary'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q4. Affichage des infromations récupérées sur une carte\n",
    "\n",
    "1. Afficher les villes sous la forme de points en 2D (scatter plot sur les 7 villes)\n",
    "1. Ajouter les noms des villes\n",
    "\n",
    "       plt.text(x, y, montexte)\n",
    "1. Calculer le nombre total de jours de précipitation sur les 48 jours disponibles pour chaque ville, ainsi que le total de pluie en mm\n",
    "\n",
    "    C'est à vous de trouver la bonne colonne à exploiter\n",
    "    ATTENTION: La colonne est définie en chaine de caractères...\n",
    "    Pour faire des opérations, il faudra convertir en utilisant la syntaxe suivante:\n",
    "    \n",
    "        np.array(df['NAME_COLONNE'], dtype=float)\n",
    "    \n",
    "    Afin de gagner du temps dans la suite, il est intéressant de modifier la défintion de la colonne:\n",
    "    \n",
    "        df['NAME_COLONNE'] = np.array(df['NAME_COLONNE'], dtype=float)\n",
    "        df['NAME_COLONNE'] = np.array(df['NAME_COLONNE'], dtype=int) # variables entières comme date.mon\n",
    "\n",
    "    Utiliser cette syntaxe sur la pluie, la neige, les mois (date.mon), le brouillard... Toutes les variables susceptibes de vous intéresser\n",
    "1. Extrapoler ces chiffres sur l'année pour chaque ville (règle de proportionnalité)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Diviser le code de la réponse pour que ce soit plus simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q5. Etude des corrélations\n",
    "Pour les deux questions suivantes, vous utiliserez une boucle for sur une liste contenant les noms de variables afin de rendre le code plus élégant. Pour le coefficient de corrélation, vous utiliserez directement:\n",
    "    \n",
    "    np.corrcoef(X,Y)[0,1]\n",
    "\n",
    "1. Comparons les coefficients de corrélation entre la variable 'pluie' et les variables 'mois', 'latitude' et 'longitude'\n",
    "\n",
    "1. Comparons ensuite la variable donnant les mm de pluie par rapport aux variables de références"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.         -0.01290968  0.12103144]\n",
      " [-0.01290968  1.         -0.04056042]\n",
      " [ 0.12103144 -0.04056042  1.        ]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "X = np.random.rand(100, 3)\n",
    "\n",
    "print(np.corrcoef(X.T))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q6. Etude de la distribution jointe P('precipm', 'date.mon')\n",
    "Nous distinguerons 10 niveaux de précipitation. On ne discrétisera pas les mois (qui le sont déjà)\n",
    "\n",
    "1. Afficher un scatter plot des deux variables\n",
    "\n",
    "1. Calculer la loi jointe et l'afficher avec imshow\n",
    "\n",
    "    ATTENTION aux indices... Les mois vont de 1 à 12, les matrices de 0 à 11.\n",
    "\n",
    "1. Calculer la distribution conditionnelle p('Precipm' | 'date.mon') et l'afficher\n",
    "\n",
    "    Note: cette distribution conditionnelle n'apporte pas vraiment d'information... C'est juste un exercice scolaire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# affichage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calcul de la loi jointe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calcul de la loi conditionnelle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q7. Proposer une étude amusante ou une illustration originale sur ces données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
